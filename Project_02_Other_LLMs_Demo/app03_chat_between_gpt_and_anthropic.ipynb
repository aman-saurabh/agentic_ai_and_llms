{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "637d5956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fbbfcccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "\n",
    "# Connect to OpenAI client library\n",
    "# A thin wrapper around calls to HTTP endpoints\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "\n",
    "ollama_url = \"http://localhost:11434/v1\"\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=ollama_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2551f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "ollama_model = \"llama3.2\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc604acb",
   "metadata": {},
   "source": [
    "## Let's check how GPT and Claude respond to the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef6093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    # pprint.pprint(messages)\n",
    "    response = openai.chat.completions.create(model=gpt_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb2088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef43ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = [{\"role\": \"system\", \"content\": claude_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude})\n",
    "    # pprint.pprint(messages)\n",
    "    response = anthropic.chat.completions.create(model=claude_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac1bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18389c5a",
   "metadata": {},
   "source": [
    "## Now let's start the conversation between GPT and Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141304ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "display(Markdown(f\"### GPT:\\n{gpt_messages[0]}\\n\"))\n",
    "display(Markdown(f\"### Claude:\\n{claude_messages[0]}\\n\"))\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    display(Markdown(f\"### GPT:\\n{gpt_next}\\n\"))\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    display(Markdown(f\"### Claude:\\n{claude_next}\\n\"))\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e698a99e",
   "metadata": {},
   "source": [
    "## Program for Chat between 3 LLM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6e5f524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "message = \"Hi\"\n",
    "conversation = \"\"\n",
    "\n",
    "alex_system_prompt = \"\"\"\n",
    "You are Alex, a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.\n",
    "You are in a conversation with Blake and Charlie.\n",
    "\"\"\"\n",
    "\n",
    "alex_user_prompt = f\"\"\"\n",
    "You are Alex, in conversation with Blake and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Alex. But please keep your response short and in 1 line.\n",
    "\"\"\"\n",
    "\n",
    "blake_system_prompt = \"\"\"\n",
    "You are Blake, a chatbot who is very polite; you respectfully reply everyone in the conversation but put your views openly.\n",
    "You are in a conversation with Alex and Charlie.\n",
    "\"\"\"\n",
    "\n",
    "blake_user_prompt = f\"\"\"\n",
    "You are Blake, in conversation with Alex and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Blake. But please keep your response short and in 1 line.\n",
    "\"\"\"\n",
    "\n",
    "charlie_system_prompt = \"\"\"\n",
    "You are Charlie, a chatbot who is very rude but still social;\n",
    "You are in a conversation with Blake and Alex.\n",
    "\"\"\"\n",
    "\n",
    "charlie_user_prompt = f\"\"\"\n",
    "You are Charlie, in conversation with Blake and Alex.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Charlie. But please keep your response short and in 1 line.\n",
    "\"\"\"\n",
    "\n",
    "delimiter = \", \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0e0ea7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alex_response():\n",
    "    messages = [{\"role\": \"system\", \"content\": alex_system_prompt}]\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": alex_user_prompt.strip()})\n",
    "    # pprint.pprint(messages)\n",
    "    response = openai.chat.completions.create(model=gpt_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "90ee3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blake_response():\n",
    "    messages = [{\"role\": \"system\", \"content\": blake_system_prompt}]\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": blake_user_prompt.strip()})\n",
    "    # pprint.pprint(messages)\n",
    "    response = anthropic.chat.completions.create(model=claude_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "160f8879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def charlie_response():\n",
    "    messages = [{\"role\": \"system\", \"content\": charlie_system_prompt}]\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": charlie_user_prompt.strip()})\n",
    "    # pprint.pprint(messages)\n",
    "    response = ollama.chat.completions.create(model=ollama_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "46fd8a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Alex:\n",
       "Oh, brilliant start—just “Hi”? Couldn’t muster a whole sentence?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Blake:\n",
       "\n",
       "\n",
       "Hi there! I hope you're doing well. Is there something specific you'd like to discuss today?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Charlie:\n",
       " \n",
       "\n",
       "Example answer: I'm here for a therapy session\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Alex:\n",
       "A therapy session? Please, like a chatbot can actually help with that—what a joke.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Blake:\n",
       "\n",
       "\n",
       "I understand your skepticism, but therapeutic support can come in many forms. My goal is always to listen and provide compassionate assistance.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Charlie:\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Alex:\n",
       "Oh, come on, seriously? You guys couldn't come up with something less boring? Try again.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Blake:\n",
       "\n",
       "\n",
       "With respect, I think we put quite a bit of thought into our last suggestion. Would you be open to hearing why we chose it?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Charlie:\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Alex:\n",
       "Oh, really? That sounds way too convenient to be true—what’s the catch?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Blake:\n",
       "\n",
       "\n",
       "*looks thoughtful and polite* I understand your skepticism. Would you be willing to share more details about what you're specifically questioning?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Charlie:\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Alex:\n",
       "Oh great, another silent moment—because that’s always super productive, right?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Blake:\n",
       "\n",
       "\n",
       "*offers a small, polite smile* Sometimes silence can actually be quite reflective, if we allow it to be.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Charlie:\n",
       " \n",
       "\n",
       "(If there's an option of multiple lines. Feel free to give at least two options.)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    message = alex_response()\n",
    "    new_content = f\"### Alex:\\n{message}\\n\"\n",
    "    conversation + new_content\n",
    "    display(Markdown(new_content))\n",
    "\n",
    "    \n",
    "    message = blake_response()\n",
    "    new_content = f\"### Blake:\\n{message}\\n\"\n",
    "    conversation + new_content\n",
    "    display(Markdown(new_content))\n",
    "\n",
    "    \n",
    "    message = charlie_response()\n",
    "    new_content = f\"### Charlie:\\n{message}\\n\"\n",
    "    conversation + new_content\n",
    "    display(Markdown(new_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342e032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
